import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
warnings.filterwarnings('ignore')

from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import classification_report, confusion_matrix

print('Libraries imported successfully!')
dataset_path = 'Fruit And Vegetable Diseases Dataset'

classes = sorted(os.listdir(dataset_path))
print(f'Total Classes: {len(classes)}')
print(classes)
# Build DataFrame with image paths and labels
data = []
for class_name in classes:
    class_path = os.path.join(dataset_path, class_name)
    if os.path.isdir(class_path):
        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)
            data.append([img_path, class_name])

df = pd.DataFrame(data, columns=['image_path', 'label'])
print(f'Total Images: {df.shape[0]}')
print(f'Total Classes: {df["label"].nunique()}')
df.head()
# Class distribution
plt.figure(figsize=(18, 6))
df['label'].value_counts().plot(kind='bar', color='steelblue')
plt.title('Class Distribution - Number of Images per Class')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
# Sample images from dataset
fig, axes = plt.subplots(4, 7, figsize=(20, 12))
axes = axes.flatten()

for i, class_name in enumerate(classes):
    class_path = os.path.join(dataset_path, class_name)
    img_file = os.listdir(class_path)[0]
    img = load_img(os.path.join(class_path, img_file), target_size=(100, 100))
    axes[i].imshow(img)
    axes[i].set_title(class_name, fontsize=8)
    axes[i].axis('off')

plt.suptitle('Sample Images from Each Class', fontsize=14)
plt.tight_layout()
plt.show()
# Fresh vs Rotten distribution
df['condition'] = df['label'].apply(lambda x: 'Fresh' if 'fresh' in x.lower() else 'Rotten')

plt.figure(figsize=(6, 5))
df['condition'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['#66b3ff','#ff9999'],
                                     startangle=90, shadow=True)
plt.title('Fresh vs Rotten Distribution')
plt.ylabel('')
plt.show()

print(df['condition'].value_counts())
IMG_SIZE = (224, 224)
BATCH_SIZE = 32

# Data augmentation for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

# Training data
train_generator = train_datagen.flow_from_directory(
    dataset_path,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

# Validation data
val_generator = train_datagen.flow_from_directory(
    dataset_path,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

print(f'Training samples: {train_generator.samples}')
print(f'Validation samples: {val_generator.samples}')
print(f'Classes: {train_generator.class_indices}')
NUM_CLASSES = len(classes)

# Load VGG16 pre-trained on ImageNet (without top layers)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom classification layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(NUM_CLASSES, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
# Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

checkpoint = ModelCheckpoint(
    'healthy_vs_rotten.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

# Train
history = model.fit(
    train_generator,
    epochs=20,
    validation_data=val_generator,
    callbacks=[early_stop, checkpoint],
    verbose=1
)

print('Training Complete!')
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Accuracy
axes[0].plot(history.history['accuracy'], label='Train Accuracy', color='blue')
axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', color='orange')
axes[0].set_title('Model Accuracy')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend()
axes[0].grid(True)

# Loss
axes[1].plot(history.history['loss'], label='Train Loss', color='blue')
axes[1].plot(history.history['val_loss'], label='Val Loss', color='orange')
axes[1].set_title('Model Loss')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()
# Evaluate on validation set
loss, accuracy = model.evaluate(val_generator)
print(f'Validation Loss: {loss:.4f}')
print(f'Validation Accuracy: {accuracy*100:.2f}%')
# Predictions
val_generator.reset()
y_pred = model.predict(val_generator, verbose=1)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = val_generator.classes

class_labels = list(val_generator.class_indices.keys())

# Classification Report
print('\nClassification Report:')
print(classification_report(y_true, y_pred_classes, target_names=class_labels))
# Confusion Matrix
cm = confusion_matrix(y_true, y_pred_classes)

plt.figure(figsize=(20, 16))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()
def predict_image(img_path, model, class_labels):
    img = load_img(img_path, target_size=(224, 224))
    img_array = img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    prediction = model.predict(img_array)
    predicted_class = class_labels[np.argmax(prediction)]
    confidence = np.max(prediction) * 100

    plt.imshow(img)
    plt.title(f'Predicted: {predicted_class}\nConfidence: {confidence:.2f}%')
    plt.axis('off')
    plt.show()

    return predicted_class, confidence

# Test with any image from dataset
sample_class = classes[0]
sample_img = os.listdir(os.path.join(dataset_path, sample_class))[0]
sample_path = os.path.join(dataset_path, sample_class, sample_img)

pred_class, conf = predict_image(sample_path, model, class_labels)
print(f'Predicted Class: {pred_class}')
print(f'Confidence: {conf:.2f}%')
model.save('healthy_vs_rotten.h5')
print('Model saved as healthy_vs_rotten.h5')

# Save class labels for Flask app
import json
with open('class_labels.json', 'w') as f:
    json.dump(class_labels, f)
print('Class labels saved as class_labels.json')
